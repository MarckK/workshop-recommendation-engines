{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import Row, SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"recommender\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MovieLens dataset\n",
    "\n",
    "The MovieLens dataset (already available with this notebook, originally [here](https://grouplens.org/datasets/movielens/)) is a popular dataset used in recommendation engines research.\n",
    "In this workshop we will use the \"small\" variant, consisting of 20000263 ratings for 27278 movies by 138493 users.\n",
    "This dataset consist of several CSV files, namely\n",
    " * A ratings data file (`ratings.csv`)\n",
    " * A tags data file (`tags.csv`)\n",
    " * A movies data file (`movies.csv`)\n",
    " * A links data file (`links.csv`)\n",
    " * A tag genome (`genome-scores.csv` and `genome-tags.csv`)\n",
    " \n",
    " For the scope of this workshop, we are mainly interest in the rating and movies data files, which have, respectively, the following structure:\n",
    " The ratings data:\n",
    " * `userId`, a unique numerical user id\n",
    " * `movieId`, a unique numerical movie id\n",
    " * `rating`, a numerical rating, as an integer from $0$ to $5$\n",
    " * `timestamp`, a numerical timestamp (not used in this exercise)\n",
    " The movies data:\n",
    " * `movieId`, as above\n",
    " * `title`, the movie title\n",
    " * `genres` a string label with the movie genre (as listed [here](http://files.grouplens.org/datasets/movielens/ml-20m-README.html)).\n",
    " \n",
    " ### Loading the data\n",
    " \n",
    " We start by using Apache Spark's built in functionality to load text files into the Spark cluster, `.textFile()` (documented [here](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.SparkContext.textFile)), which will return an `RDD` of strings.\n",
    " Since `.textFile()` will read each row in the CSV file as an RDD entry, we further process it by then splitting each row into a list of CSV entries and keep only the relevant fields (i.e. `userId`, `movieId` and `rating` for the ratings, and `movieId` and `title` for the movies file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, header, token_fun):\n",
    "    \"\"\"Loads the CSV file, strips the header and keeps only the specified fields\"\"\"\n",
    "    rows = spark.sparkContext.textFile(path).filter(lambda x: x!=header).map(lambda x: x.split(\",\")).map(token_fun)\n",
    "    return spark.createDataFrame(rows)\n",
    "    \n",
    "    \n",
    "\n",
    "movies = load_data('ml-latest-small/movies.csv', \n",
    "                   'movieId,title,genres', \n",
    "                   lambda tokens: Row(item=int(tokens[0]),title=tokens[1]))\n",
    "\n",
    "ratings = load_data('ml-latest-small/ratings.csv', \n",
    "                    'userId,movieId,rating,timestamp', \n",
    "                    lambda tokens: Row(user=int(tokens[0]), item=int(tokens[1]), rating=float(tokens[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies: 9742\n",
      "Number of ratings: 100836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(item=1, title='Toy Story (1995)'),\n",
       " Row(item=2, title='Jumanji (1995)'),\n",
       " Row(item=3, title='Grumpier Old Men (1995)'),\n",
       " Row(item=4, title='Waiting to Exhale (1995)'),\n",
       " Row(item=5, title='Father of the Bride Part II (1995)')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of movies: {}\".format(movies.count()))\n",
    "print(\"Number of ratings: {}\".format(ratings.count()))\n",
    "movies.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start by exploring the data.\n",
    "Let's look at the ratings distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x122c169b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEAtJREFUeJzt3W2sXWWZxvH/NQUcgxqqdJqmLVMyNibVZCo2QIKZOJophTFTTIwBMtAYxppYEsyYjOgXHJVEP6gTEiWpQ2PJiJX4EpqZam0YEkMyFA5YgYKEE+SlTaXVomhMNOA9H87TsNvntOdwTtt18Px/ycpe+17PWvvei4brrJe9d6oKSZJG/cXQDUiS5h7DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSZ0zhm5gps4999xasWLF0G1I0mvKgw8++MuqWjTVuNdsOKxYsYKxsbGh25Ck15Qkz0xnnKeVJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEmd1+wnpCX9+blj97OT1q++6LzT3Ik8cpAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdaYMhyTLk9yT5LEke5Pc0OqfSbI/yZ42XT6yzqeSjCd5IsmlI/V1rTae5MaR+vlJdrf6t5OcdbLfqCRp+qZz5PAS8ImqWgVcDGxKsqot+0pVrW7TDoC27Erg7cA64GtJFiRZAHwVuAxYBVw1sp0vtm29FXgBuO4kvT9J0gxMGQ5VdaCqHmrzvwUeB5aeYJX1wLaq+kNV/RwYBy5s03hVPVVVfwS2AeuTBHgv8J22/lbgipm+IUnS7L2qaw5JVgDvBHa30vVJHk6yJcnCVlsKPDey2r5WO179LcCvq+qlY+qSpIFMOxySvAH4LvDxqnoRuBX4G2A1cAD40inp8OgeNiYZSzJ26NChU/1ykjRvTSsckpzJRDB8s6q+B1BVz1fVy1X1J+DrTJw2AtgPLB9ZfVmrHa/+K+CcJGccU+9U1eaqWlNVaxYtWjSd1iVJMzCdu5UC3AY8XlVfHqkvGRn2AeDRNr8duDLJ65KcD6wE7gceAFa2O5POYuKi9faqKuAe4INt/Q3AXbN7W5Kk2ZjOj/1cAlwDPJJkT6t9mom7jVYDBTwNfBSgqvYmuRN4jIk7nTZV1csASa4HdgILgC1Vtbdt75PAtiSfB37CRBhJkgYyZThU1b1AJlm04wTr3AzcPEl9x2TrVdVTvHJaSpI0MD8hLUnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpM6U4ZBkeZJ7kjyWZG+SG1r9zUl2JXmyPS5s9SS5Jcl4koeTXDCyrQ1t/JNJNozU35XkkbbOLUlyKt6sJGl6pnPk8BLwiapaBVwMbEqyCrgRuLuqVgJ3t+cAlwEr27QRuBUmwgS4CbgIuBC46UigtDEfGVlv3ezfmiRppqYMh6o6UFUPtfnfAo8DS4H1wNY2bCtwRZtfD9xeE+4DzkmyBLgU2FVVh6vqBWAXsK4te1NV3VdVBdw+si1J0gDOeDWDk6wA3gnsBhZX1YG26BfA4ja/FHhuZLV9rXai+r5J6pO9/kYmjkY477zzXk3r0px1x+5nJ61ffZH/xjWcaV+QTvIG4LvAx6vqxdFl7S/+Osm9dapqc1Wtqao1ixYtOtUvJ0nz1rTCIcmZTATDN6vqe638fDslRHs82Or7geUjqy9rtRPVl01SlyQNZDp3KwW4DXi8qr48smg7cOSOow3AXSP1a9tdSxcDv2mnn3YCa5MsbBei1wI727IXk1zcXuvakW1JkgYwnWsOlwDXAI8k2dNqnwa+ANyZ5DrgGeBDbdkO4HJgHPg98GGAqjqc5HPAA23cZ6vqcJv/GPAN4PXAD9okSRrIlOFQVfcCx/vcwfsmGV/ApuNsawuwZZL6GPCOqXqRJJ0efkJaktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktSZMhySbElyMMmjI7XPJNmfZE+bLh9Z9qkk40meSHLpSH1dq40nuXGkfn6S3a3+7SRnncw3KEl69aZz5PANYN0k9a9U1eo27QBIsgq4Enh7W+drSRYkWQB8FbgMWAVc1cYCfLFt663AC8B1s3lDkqTZmzIcqurHwOFpbm89sK2q/lBVPwfGgQvbNF5VT1XVH4FtwPokAd4LfKetvxW44lW+B0nSSTabaw7XJ3m4nXZa2GpLgedGxuxrtePV3wL8uqpeOqYuSRrQTMPhVuBvgNXAAeBLJ62jE0iyMclYkrFDhw6djpeUpHlpRuFQVc9X1ctV9Sfg60ycNgLYDywfGbqs1Y5X/xVwTpIzjqkf73U3V9WaqlqzaNGimbQuSZqGGYVDkiUjTz8AHLmTaTtwZZLXJTkfWAncDzwArGx3Jp3FxEXr7VVVwD3AB9v6G4C7ZtKTJOnkOWOqAUm+BbwHODfJPuAm4D1JVgMFPA18FKCq9ia5E3gMeAnYVFUvt+1cD+wEFgBbqmpve4lPAtuSfB74CXDbSXt3kqQZmTIcquqqScrH/R94Vd0M3DxJfQewY5L6U7xyWkqSNAf4CWlJUsdwkCR1DAdJUsdwkCR1DAdJUsdwkCR1DAdJUsdwkCR1DAdJUmfKT0hLmn/u2P3spPWrLzrvNHcyfw3938AjB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHWmDIckW5IcTPLoSO3NSXYlebI9Lmz1JLklyXiSh5NcMLLOhjb+ySQbRurvSvJIW+eWJDnZb1KS9OpM58jhG8C6Y2o3AndX1Urg7vYc4DJgZZs2ArfCRJgANwEXARcCNx0JlDbmIyPrHftakqTTbMpwqKofA4ePKa8Htrb5rcAVI/Xba8J9wDlJlgCXAruq6nBVvQDsAta1ZW+qqvuqqoDbR7YlSRrITK85LK6qA23+F8DiNr8UeG5k3L5WO1F93yR1SdKAZn1Buv3FXyehlykl2ZhkLMnYoUOHTsdLStK8NNNweL6dEqI9Hmz1/cDykXHLWu1E9WWT1CdVVZurak1VrVm0aNEMW5ckTWWm4bAdOHLH0QbgrpH6te2upYuB37TTTzuBtUkWtgvRa4GdbdmLSS5udyldO7ItSdJAzphqQJJvAe8Bzk2yj4m7jr4A3JnkOuAZ4ENt+A7gcmAc+D3wYYCqOpzkc8ADbdxnq+rIRe6PMXFH1OuBH7RJkjSgKcOhqq46zqL3TTK2gE3H2c4WYMsk9THgHVP1IUk6ffyEtCSpYzhIkjqGgySpYzhIkjqGgySpYzhIkjpT3soqSfPZHbufnbR+9UXnneZOTi+PHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktTxVlYNYr7eHii9VnjkIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnq+HsOUuNvTEivmNWRQ5KnkzySZE+SsVZ7c5JdSZ5sjwtbPUluSTKe5OEkF4xsZ0Mb/2SSDbN7S5Kk2ToZp5X+vqpWV9Wa9vxG4O6qWgnc3Z4DXAasbNNG4FaYCBPgJuAi4ELgpiOBIkkaxqm45rAe2NrmtwJXjNRvrwn3AeckWQJcCuyqqsNV9QKwC1h3CvqSJE3TbMOhgB8leTDJxlZbXFUH2vwvgMVtfinw3Mi6+1rtePVOko1JxpKMHTp0aJatS5KOZ7YXpN9dVfuT/BWwK8nPRhdWVSWpWb7G6PY2A5sB1qxZc9K2K0k62qyOHKpqf3s8CHyfiWsGz7fTRbTHg234fmD5yOrLWu14dUnSQGYcDknOTvLGI/PAWuBRYDtw5I6jDcBdbX47cG27a+li4Dft9NNOYG2She1C9NpWkyQNZDanlRYD309yZDt3VNUPkzwA3JnkOuAZ4ENt/A7gcmAc+D3wYYCqOpzkc8ADbdxnq+rwLPqSJM3SjMOhqp4C/naS+q+A901SL2DTcba1Bdgy014kSSeXX58hSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSerM9vcc9Cr4A/aSXis8cpAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdfycwzznZy8kTcYjB0lSx3CQJHXm5WklT6VI0ol55CBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqTOnAmHJOuSPJFkPMmNQ/cjSfPZnAiHJAuArwKXAauAq5KsGrYrSZq/5kQ4ABcC41X1VFX9EdgGrB+4J0mat+ZKOCwFnht5vq/VJEkDSFUN3QNJPgisq6p/ac+vAS6qquuPGbcR2Nievg144rQ2evKdC/xy6CbmCPfF0dwfR3N/vGK2++Kvq2rRVIPmyncr7QeWjzxf1mpHqarNwObT1dSplmSsqtYM3cdc4L44mvvjaO6PV5yufTFXTis9AKxMcn6Ss4Arge0D9yRJ89acOHKoqpeSXA/sBBYAW6pq78BtSdK8NSfCAaCqdgA7hu7jNPuzOUV2Ergvjub+OJr74xWnZV/MiQvSkqS5Za5cc5AkzSGGwwCSbElyMMmjQ/cytCTLk9yT5LEke5PcMHRPQ0ryl0nuT/LTtj/+feiehpZkQZKfJPnvoXsZWpKnkzySZE+SsVP6Wp5WOv2S/B3wO+D2qnrH0P0MKckSYElVPZTkjcCDwBVV9djArQ0iSYCzq+p3Sc4E7gVuqKr7Bm5tMEn+FVgDvKmq3j90P0NK8jSwpqpO+Wc+PHIYQFX9GDg8dB9zQVUdqKqH2vxvgceZx5+Orwm/a0/PbNO8/QsuyTLgH4H/HLqX+cZw0JyRZAXwTmD3sJ0Mq51G2QMcBHZV1XzeH/8B/Bvwp6EbmSMK+FGSB9s3RpwyhoPmhCRvAL4LfLyqXhy6nyFV1ctVtZqJbwq4MMm8PPWY5P3Awap6cOhe5pB3V9UFTHyD9aZ2ivqUMBw0uHZu/bvAN6vqe0P3M1dU1a+Be4B1Q/cykEuAf2rn2bcB703yX8O2NKyq2t8eDwLfZ+IbrU8Jw0GDahdgbwMer6ovD93P0JIsSnJOm3898A/Az4btahhV9amqWlZVK5j4Sp3/rap/HritwSQ5u920QZKzgbXAKbvj0XAYQJJvAf8HvC3JviTXDd3TgC4BrmHir8I9bbp86KYGtAS4J8nDTHzn2K6qmve3cAqAxcC9SX4K3A/8T1X98FS9mLeySpI6HjlIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySp8/+3BWoM8X840QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "r = ratings.select('rating').collect()\n",
    "sns.distplot(r, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "To train Spark's ALS model, we need to perform a few steps.\n",
    "The first step is to define a metric to quantify \"how good\" our model is. This is done by chosing an error measure, which typically is the _mean squared error_ (MSE). The mean square error is formally defined by\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N}\\sum_{i=1}^{N}\\epsilon_i^2, \\qquad \\epsilon_i = r_i - \\hat{r}_i.\n",
    "$$\n",
    "\n",
    "That is, we first calculate the difference, $\\epsilon_i$, between each pair of true rating ($r_i$) and it's corresponding _predicted_ rating ($\\hat{r}_i$) and then calculated the average of the square of these errors. As example, consider a list of ratings `r1` and corresponding predictiong `r2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = [4.5, 3.0, 1.0, 5.0, 4.0]\n",
    "r2 = [4.56, 2.9, 1.1, 4.9, 3.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.13471999999999992\n"
     ]
    }
   ],
   "source": [
    "def mse(r, p):\n",
    "    return sum([(pair[0] - pair[1])**2 for pair in zip(r, p)]) / len(r)\n",
    "\n",
    "print(\"MSE = {}\".format(mse(r1, r2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've established our error measure, we can proceed to the next step.\n",
    "We now need to split our data into a _training_, _test_ and _validation_ subsets.\n",
    "We will use respectively the proportions 63%, 18% and 18%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size = 63952\n",
      "Validation dataset size = 18559\n",
      "Test dataset size = 18325\n"
     ]
    }
   ],
   "source": [
    "def split_sets(ratings, proportions):\n",
    "    split = ratings.randomSplit(proportions, seed=42)\n",
    "    return {'training': split[0], 'validation': split[1], 'test': split[2]}\n",
    "    \n",
    "sets = split_sets(ratings, [0.63212056, 0.1839397, 0.1839397])\n",
    "print(\"Training dataset size = {}\".format(sets['training'].count()))\n",
    "print(\"Validation dataset size = {}\".format(sets['validation'].count()))\n",
    "print(\"Test dataset size = {}\".format(sets['test'].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation\n",
    "\n",
    "We add the `coldStartStrategy` to the ALS class so that we automatically drop any `NaN` predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "als = ALS(coldStartStrategy=\"drop\")\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [6, 8, 10, 12]) \\\n",
    "    .addGrid(als.maxIter,[6, 8, 10, 12]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"mse\",\n",
    "    labelCol=\"rating\")\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    ")\n",
    "\n",
    "model = tvs.fit(sets['training'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------------------------------+----------+------+\n",
      "|user|title                                           |prediction|rating|\n",
      "+----+------------------------------------------------+----------+------+\n",
      "|599 |Othello (1995)                                  |3.0       |2.5   |\n",
      "|40  |Othello (1995)                                  |4.0       |4.0   |\n",
      "|68  |Othello (1995)                                  |3.0       |3.0   |\n",
      "|606 |\"City of Lost Children                          |4.0       |4.5   |\n",
      "|599 |\"City of Lost Children                          |3.0       |3.5   |\n",
      "|171 |\"City of Lost Children                          |5.0       |5.0   |\n",
      "|95  |\"City of Lost Children                          |4.0       |5.0   |\n",
      "|323 |\"City of Lost Children                          |4.0       |3.5   |\n",
      "|221 |\"City of Lost Children                          |4.0       |4.5   |\n",
      "|600 |\"City of Lost Children                          |4.0       |4.5   |\n",
      "|66  |\"City of Lost Children                          |4.0       |4.5   |\n",
      "|602 |In the Line of Fire (1993)                      |4.0       |5.0   |\n",
      "|476 |In the Line of Fire (1993)                      |4.0       |4.0   |\n",
      "|599 |In the Line of Fire (1993)                      |3.0       |2.5   |\n",
      "|305 |In the Line of Fire (1993)                      |4.0       |4.0   |\n",
      "|57  |In the Line of Fire (1993)                      |4.0       |3.0   |\n",
      "|235 |In the Line of Fire (1993)                      |4.0       |3.0   |\n",
      "|290 |In the Line of Fire (1993)                      |4.0       |4.0   |\n",
      "|404 |In the Line of Fire (1993)                      |4.0       |3.0   |\n",
      "|288 |In the Line of Fire (1993)                      |3.0       |3.0   |\n",
      "|480 |In the Line of Fire (1993)                      |3.0       |3.0   |\n",
      "|469 |In the Line of Fire (1993)                      |3.0       |3.0   |\n",
      "|109 |In the Line of Fire (1993)                      |3.0       |3.0   |\n",
      "|11  |In the Line of Fire (1993)                      |4.0       |4.0   |\n",
      "|274 |Paulie (1998)                                   |2.0       |3.5   |\n",
      "|217 |Paulie (1998)                                   |2.0       |1.0   |\n",
      "|599 |In the Heat of the Night (1967)                 |3.0       |3.5   |\n",
      "|140 |In the Heat of the Night (1967)                 |4.0       |4.0   |\n",
      "|290 |In the Heat of the Night (1967)                 |4.0       |5.0   |\n",
      "|188 |In the Heat of the Night (1967)                 |4.0       |4.0   |\n",
      "|27  |\"Computer Wore Tennis Shoes                     |3.0       |2.0   |\n",
      "|19  |\"Computer Wore Tennis Shoes                     |2.0       |2.0   |\n",
      "|603 |Planet of the Apes (1968)                       |4.0       |4.0   |\n",
      "|479 |Planet of the Apes (1968)                       |4.0       |3.0   |\n",
      "|607 |Planet of the Apes (1968)                       |4.0       |5.0   |\n",
      "|387 |Planet of the Apes (1968)                       |3.0       |4.0   |\n",
      "|290 |Planet of the Apes (1968)                       |4.0       |4.0   |\n",
      "|361 |Planet of the Apes (1968)                       |3.0       |2.0   |\n",
      "|75  |Planet of the Apes (1968)                       |3.0       |1.5   |\n",
      "|265 |Planet of the Apes (1968)                       |4.0       |4.0   |\n",
      "|546 |Planet of the Apes (1968)                       |4.0       |3.0   |\n",
      "|199 |Planet of the Apes (1968)                       |4.0       |4.0   |\n",
      "|186 |Planet of the Apes (1968)                       |4.0       |5.0   |\n",
      "|483 |Planet of the Apes (1968)                       |4.0       |3.0   |\n",
      "|332 |Kagemusha (1980)                                |3.0       |4.5   |\n",
      "|603 |Kagemusha (1980)                                |3.0       |4.0   |\n",
      "|217 |North Dallas Forty (1979)                       |2.0       |4.0   |\n",
      "|504 |Serendipity (2001)                              |3.0       |2.5   |\n",
      "|484 |Serendipity (2001)                              |4.0       |4.5   |\n",
      "|381 |Serendipity (2001)                              |3.0       |3.0   |\n",
      "|200 |Serendipity (2001)                              |4.0       |4.0   |\n",
      "|105 |Serendipity (2001)                              |3.0       |3.5   |\n",
      "|483 |Serendipity (2001)                              |3.0       |3.5   |\n",
      "|438 |Rapid Fire (1992)                               |NaN       |3.5   |\n",
      "|34  |Once Upon a Time in China (Wong Fei Hung) (1991)|2.0       |4.5   |\n",
      "|111 |Pumping Iron (1977)                             |2.0       |4.0   |\n",
      "|318 |\"Human Condition I                              |NaN       |4.5   |\n",
      "|610 |\"Happiness of the Katakuris                     |4.0       |4.0   |\n",
      "|387 |\"Host                                           |4.0       |4.0   |\n",
      "|380 |\"Host                                           |4.0       |4.0   |\n",
      "|599 |Step Brothers (2008)                            |2.0       |2.5   |\n",
      "|356 |Step Brothers (2008)                            |4.0       |3.0   |\n",
      "|73  |Step Brothers (2008)                            |4.0       |4.5   |\n",
      "|477 |Step Brothers (2008)                            |3.0       |4.0   |\n",
      "|18  |Step Brothers (2008)                            |3.0       |3.0   |\n",
      "|232 |Surrogates (2009)                               |3.0       |4.0   |\n",
      "|292 |Surrogates (2009)                               |3.0       |3.0   |\n",
      "|590 |Up in the Air (2009)                            |3.0       |4.0   |\n",
      "|495 |Up in the Air (2009)                            |4.0       |4.5   |\n",
      "|249 |Up in the Air (2009)                            |4.0       |4.0   |\n",
      "|352 |Up in the Air (2009)                            |4.0       |4.0   |\n",
      "|222 |New Year's Eve (2011)                           |2.0       |1.5   |\n",
      "|41  |\"Hunt                                           |3.0       |4.0   |\n",
      "|105 |\"Hunt                                           |4.0       |5.0   |\n",
      "|298 |\"Way                                            |1.0       |3.0   |\n",
      "|154 |Ender's Game (2013)                             |4.0       |4.0   |\n",
      "|561 |Ender's Game (2013)                             |3.0       |2.5   |\n",
      "|610 |Dallas Buyers Club (2013)                       |5.0       |4.0   |\n",
      "|105 |Dallas Buyers Club (2013)                       |5.0       |4.5   |\n",
      "|298 |Dallas Buyers Club (2013)                       |3.0       |2.5   |\n",
      "|424 |Dallas Buyers Club (2013)                       |4.0       |4.0   |\n",
      "|539 |Bio-Dome (1996)                                 |3.0       |4.5   |\n",
      "|19  |Bio-Dome (1996)                                 |2.0       |2.0   |\n",
      "|277 |Bio-Dome (1996)                                 |2.0       |3.0   |\n",
      "|337 |Bio-Dome (1996)                                 |3.0       |3.0   |\n",
      "|294 |Bio-Dome (1996)                                 |3.0       |3.0   |\n",
      "|599 |\"Scarlet Letter                                 |1.0       |1.5   |\n",
      "|414 |\"Scarlet Letter                                 |1.0       |2.0   |\n",
      "|597 |Blade Runner (1982)                             |5.0       |4.0   |\n",
      "|385 |Blade Runner (1982)                             |4.0       |5.0   |\n",
      "|330 |Blade Runner (1982)                             |4.0       |2.0   |\n",
      "|246 |Blade Runner (1982)                             |4.0       |5.0   |\n",
      "|274 |Blade Runner (1982)                             |4.0       |2.5   |\n",
      "|292 |Blade Runner (1982)                             |3.0       |3.5   |\n",
      "|339 |Blade Runner (1982)                             |5.0       |5.0   |\n",
      "|41  |Blade Runner (1982)                             |3.0       |2.0   |\n",
      "|489 |Blade Runner (1982)                             |3.0       |4.0   |\n",
      "|590 |Blade Runner (1982)                             |4.0       |4.5   |\n",
      "|17  |Blade Runner (1982)                             |4.0       |3.5   |\n",
      "|610 |Blade Runner (1982)                             |4.0       |5.0   |\n",
      "+----+------------------------------------------------+----------+------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "prediction = model.transform(sets['test'])\n",
    "\n",
    "# Round floats to whole numbers\n",
    "prediction = prediction.withColumn(\"prediction\", F.abs(F.round(prediction[\"prediction\"],0)))\n",
    "\n",
    "prediction.alias('p').join(movies.alias('m'), col('p.item') == col('m.item')) \\\n",
    "    .select([col('p.user'), col('m.title'), col('p.prediction'), col('p.rating')]).show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ALSModel' object has no attribute 'predictAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-b4ffef966af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ALSModel' object has no attribute 'predictAll'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "rank = 10\n",
    "iterations = 10\n",
    "seed = 42\n",
    "model = ALS.train(sets['training'], rank, seed=seed, iterations=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = sets['validation'].map(lambda x: (x[0], x[1]))\n",
    "validation.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predictAll(validation)\n",
    "predictions.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
